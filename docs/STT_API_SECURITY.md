# 文字起こしAPIのセキュリティ検討

## 現状

- **実装状況**:
  - **リアルタイム文字起こし**: **Web Speech API** で実装済み（`speechToText.ts` の `RealtimeSpeechRecognizer`）。録音中にブラウザ内で音声をテキスト化。※ 一部ブラウザ（Chrome 等）では音声を Google のクラウドに送信する場合あり。
  - **録音Blobの文字起こし**: `transcribeAudio()` は **スタブ**（デモ用ダミー返却）。Whisper 連携は未実装。
- **要件定義**: 音声はローカル環境で処理（クラウドへの自動アップロードは行わない）を想定。
- **分析**: 文字起こしテキストは **Gemini API** に送信（突合判定）。音声データ自体は Gemini には送信しない。

## 選択肢の比較

### 0. Web Speech API（現在のリアルタイム実装）

**特徴**:
- ✅ **ブラウザ標準**: 追加ライブラリ不要
- ✅ **リアルタイム**: 録音と同時に文字起こし可能
- ❌ **クラウド送信の可能性**: Chrome 等では音声が Google のサーバーに送信される。**要件の「音声ローカル処理」に反する可能性あり**
- ⚠️ **ブラウザ依存**: 実装・精度はブラウザごとに異なる。Safari は Apple のサービスを利用する場合あり

**セキュリティ**:
- ❌ 音声が外部に送信される可能性が高く、厳密なローカル要件を満たさない
- 本番で「音声を一切クラウドに送せない」場合は、Whisper（ローカル）への置き換えを推奨

---

### 1. OpenAI Whisper（OSS）✅ 推奨（将来実装）

**特徴**:
- ✅ **完全OSS**: MITライセンス、オープンソース
- ✅ **ローカル実行可能**: クラウドにデータを送信しない
- ✅ **高精度**: 日本語を含む多言語対応、商用利用可能
- ✅ **無料**: ライセンス費用なし
- ⚠️ **処理時間**: ローカル実行のため、クラウドAPIより時間がかかる可能性

**実装方法**:
```bash
# Whisperをインストール
pip install openai-whisper

# または、より高速な実装
pip install faster-whisper
```

**セキュリティ**:
- ✅ データが外部に送信されない
- ✅ オープンソースのため、コードを確認可能
- ✅ 自己ホスト可能

---

### 2. Google Gemini API（音声対応）

**特徴**:
- ✅ **高精度**: Googleの最新モデル
- ✅ **高速**: クラウド処理のため高速
- ❌ **クラウド必須**: データがGoogleのサーバーに送信される
- ❌ **セキュリティ要件と矛盾**: 要件定義の「ローカル処理」に反する
- ⚠️ **コスト**: 従量課金

**セキュリティリスク**:
- ❌ 音声データがGoogleのサーバーに送信される
- ❌ データ保持ポリシーはGoogle側の管理
- ⚠️ 企業の機密情報（商談内容）が外部に送信される

---

### 3. Deepgram API

**特徴**:
- ✅ **高精度・高速**: リアルタイム処理に強い
- ❌ **クラウド必須**: データが外部に送信される
- ❌ **セキュリティ要件と矛盾**: ローカル処理要件に反する
- ⚠️ **コスト**: 従量課金

---

### 4. Azure Speech Services / AWS Transcribe

**特徴**:
- ✅ **エンタープライズ向け**: セキュリティ機能が充実
- ⚠️ **データリージョン指定可能**: 日本リージョンで処理可能
- ❌ **クラウド必須**: データが外部に送信される
- ⚠️ **コスト**: 従量課金
- ⚠️ **セキュリティ要件との整合性**: クラウド処理のため、要件定義の「ローカル処理」とは異なる

---

## 推奨実装: Whisper（ローカル実行）

### 理由

1. **セキュリティ要件に完全準拠**: データが外部に送信されない
2. **OSS**: コードを確認可能、透明性が高い
3. **無料**: ライセンス費用なし
4. **高精度**: 日本語の文字起こし精度が高い

### 実装アーキテクチャ

**現状**（バックエンド・Whisper なし）:

```
[ブラウザ]
   │
   ├─ 録音: MediaRecorder → [Blob（メモリ）] ─────────────────┐
   │                                                          │
   ├─ リアルタイム: マイク → [Web Speech API] → 文字起こし ────┤
   │              ※一部ブラウザでクラウド送信の可能性          │
   │                                                          │
   └─ 録音Blob: [transcribeAudio スタブ] → ダミー文字起こし ──┤
      （Whisper 未実装）                                       │
                                                              ▼
[文字起こしテキスト] → [Gemini API（ブラウザから直接）] → [分析結果]
```

**推奨**（Whisper 導入後）:

```
[ブラウザ] → [音声録音] → [WebM/MP3] → [バックエンドAPI] → [Whisper（ローカル）] → [文字起こし]
                                                                        ↓
[LLM（Gemini API または Ollama）] ← [チェックリスト突合] ← [文字起こしテキスト]
```

**Whisper 連携の注意**（将来実装時）:
- Whisper は Python ライブラリのため、Node.js から使うには:
  - オプション1: Python スクリプトを `child_process` で実行
  - オプション2: Whisper の HTTP API サーバーを立てる（例: `whisper-asr-server`）
  - オプション3: Node.js 用の Whisper ラッパーを使用

### 実装例

**現在の実装**（`src/lib/speechToText.ts`）:
- **RealtimeSpeechRecognizer**: `window.SpeechRecognition` / `webkitSpeechRecognition` を用いたリアルタイム文字起こし。`start()` / `stop()` で制御。`onresult` で確定・中間テキストを取得。
- **transcribeAudio(blob)**: 録音 Blob 用。現状は 1 秒待機後にデモ用ダミーテキストを返すスタブ。Whisper 連携時に差し替え予定。

**将来実装例**（Node.js + Python / Whisper）:

```typescript
// src/lib/transcribeWithWhisper.ts（将来）
import { exec } from 'child_process';
import { promisify } from 'util';
import * as path from 'path';

const execAsync = promisify(exec);

export async function transcribeAudioWithWhisper(audioFilePath: string): Promise<string> {
  const whisperScript = path.join(__dirname, '../../scripts/whisper_transcribe.py');
  const { stdout } = await execAsync(`python3 ${whisperScript} "${audioFilePath}"`);
  return stdout.trim();
}
```

```python
# scripts/whisper_transcribe.py（将来）
import whisper
import sys

model = whisper.load_model("base")  # または "small", "medium", "large"
result = model.transcribe(sys.argv[1], language="ja")
print(result["text"])
```

---

## セキュリティチェックリスト

### ✅ 実装時に確認すべき項目

- [ ] **音声データ**: クラウドに送信されていないか。※ **Web Speech API 使用時は Chrome 等で Google に送信される可能性あり**。厳密にローカルのみとする場合は Whisper（ローカル）へ切替を検討。
- [ ] **API キー**: `VITE_GEMINI_API_KEY` を環境変数で管理。ビルド成果物に含めない。
- [ ] **録音データ**: 音声はメモリ（Blob）または IndexedDB。一時ファイルは現状未使用（Whisper 導入時に削除ポリシーを検討）。
- [ ] **文字起こし結果**: ログに出力していないか。Gemini には送信されるため、利用規約・データ処理契約を確認。
- [ ] **エラーメッセージ**: 機密情報を含まないか。

### 🔒 追加のセキュリティ対策

1. **データ暗号化**: ローカル保存時も暗号化（オプション）
2. **アクセス制御**: 録音データへのアクセス権限管理
3. **監査ログ**: データアクセスの記録
4. **データ保持期間**: 不要になったデータの自動削除

---

## データフローとセキュリティ境界

### 現在の実装アーキテクチャ

```
[ブラウザ]
   │
   ├─ 音声: MediaRecorder → [Blob（メモリ）]  … 案件保存時は IndexedDB へ
   │
   ├─ 文字起こし（2経路）:
   │   ・リアルタイム: マイク → [Web Speech API] → テキスト
   │      ※ Chrome 等では音声が Google に送信される可能性あり
   │   ・録音Blob: [transcribeAudio スタブ] → ダミーテキスト（Whisper 未実装）
   │
   └─ [文字起こしテキスト] → [Gemini API] → [分析結果] → ChatPanel / IndexedDB
```

（バックエンド・Whisper サーバーは現状なし。）

### セキュリティ境界の明確化

#### 🟡 音声データ（現状）

- **Web Speech API（リアルタイム）**:
  - ⚠️ **一部ブラウザでクラウド送信**: Chrome 等では音声が Google のサーバーに送信される。**「音声はローカルのみ」とする要件と矛盾する可能性あり。**
  - 厳密にローカルにしたい場合は、Whisper（ローカル）への切替を推奨。
- **録音 Blob**:
  - ✅ メモリ（Blob）および IndexedDB 内に保持。音声ファイルをクラウドに送信する処理はなし。
  - `transcribeAudio` スタブは Blob を外部に送らず、ダミーテキストを返すのみ。Whisper 実装時もローカル処理を想定。

#### ✅ 音声データ（Whisper 導入後・推奨）

- **Whisper（ローカル実行）**:
  - ✅ 音声データは組織内で処理、クラウドに送信しない
  - ✅ OSS のためコード確認可能。LLM の学習に音声は使われない

#### ⚠️ 文字起こしテキスト（クラウド API 使用時）

**現状も将来も、分析には Gemini API を使用**:

- ⚠️ **文字起こしテキストは Gemini API に送信**: 突合判定のため、テキストが Google のサーバーに送信される
- ⚠️ **学習利用の可能性**: 利用規約・オプトアウトを確認。企業向け契約でデータ処理契約を検討
- ⚠️ **機密情報**: 商談内容がテキストとして外部に送信される

**対策**:
- Gemini の利用規約・データ学習オプトアウトを確認。企業向け（Google Cloud 等）のデータ処理契約を検討
- 完全ローカルにしたい場合は、LLM 分析を Ollama + Llama 3 等に切替

---

## 完全ローカル実行のオプション

### オプション1: Whisper + ローカルLLM（完全オフライン）

```
[音声] → [Whisper（ローカル）] → [テキスト] → [Ollama + Llama 3（ローカル）] → [分析結果]
```

**メリット**:
- ✅ 完全にデータが外部に送信されない
- ✅ LLMの学習にも使われない
- ✅ インターネット接続不要

**デメリット**:
- ⚠️ ローカルLLMの精度がクラウドAPIより低い可能性
- ⚠️ ローカルマシンのリソース（GPU/メモリ）が必要

### オプション2: 文字起こし（ローカル or 注意） + Gemini API

**現在の実装**:
```
[音声] → [Web Speech API（※クラウド送信の可能性）／transcribeAudioスタブ] → [テキスト] → [Gemini API] → [分析結果]
```

**Whisper 導入後の推奨**:
```
[音声] → [Whisper（ローカル）] → [テキスト] → [Gemini API] → [分析結果]
```

**メリット**:
- ✅ 録音 Blob は外部に送信しない（スタブ／将来 Whisper 時）
- ✅ 高精度な分析が可能（Gemini）

**デメリット**:
- ⚠️ **現状**: Web Speech API 使用時は音声がクラウドに送信される可能性あり
- ⚠️ 文字起こしテキストは Gemini API に送信される
- ⚠️ Gemini API の利用規約に依存

---

## 結論

### ユーザーの理解について

**「データが外部に送信されない」の意味**:

1. 🟡 **音声データ（現状）**: **Web Speech API 使用時は、Chrome 等でクラウドに送信される可能性あり**。録音 Blob はメモリ／IndexedDB 内で、外部送信なし。**Whisper 導入後は音声もローカルのみ。**
2. ✅ **LLM の学習に利用されない**: 音声データそのものを学習に回す処理はない（Web Speech / スタブ / 将来 Whisper とも）。
3. ⚠️ **文字起こしテキスト**: 現状も Gemini API に送信される（分析のため）。

### 推奨: Whisper（ローカル実行）の導入

**現状**: Web Speech API（リアルタイム）＋ transcribeAudio スタブ（録音 Blob）。音声の完全ローカル要件を満たすには不十分な場合あり。

**推奨（将来）**: Whisper をローカルで実行し、`transcribeAudio` の実装を差し替え:

- ✅ 音声データを組織内で処理（外部送信なし）
- ✅ OSS で透明性が高い。無料・商用利用可能。日本語精度も高い

### 追加のセキュリティ対策

**文字起こしテキスト**:

1. **Gemini API**: データ学習のオプトアウト、企業向けのデータ処理契約を確認
2. **完全ローカル**: Ollama + Llama 3 等で分析もローカル化すれば、文字起こしテキストの外部送信も不要
3. **データ保持**: 文字起こし・分析結果は IndexedDB に保存。不要になったデータの削除ポリシーを検討

---

## セキュリティ要件の再確認

### 要件定義の例

> 「録音データ、文字起こしデータはローカル環境で処理（クラウドへの自動アップロードは行わない）」

**現状との対応**:

| データ | 現状 | 要件との関係 |
|--------|------|--------------|
| **録音データ** | メモリ（Blob）／IndexedDB。**ただしリアルタイム時は Web Speech API がクラウド送信する可能性あり** | 🟡 録音 Blob はローカル。**Web Speech 利用時は要確認** |
| **文字起こしテキスト** | Gemini API に送信（分析のため） | ⚠️ クラウド送信のため、厳密には要件と矛盾する可能性 |

**推奨**:
- **音声を完全ローカルにする**: リアルタイム・録音Blobともに **Whisper（ローカル）** を導入し、Web Speech API および transcribeAudio スタブを置き換える
- **文字起こしテキストもローカルにする**: 分析に **Ollama + Llama 3** 等のローカル LLM を使用
- **運用で許容する場合**: 要件を「音声はローカル。文字起こしテキストは分析のため Gemini に送信する」と明確化し、利用規約・データ処理契約で担保
